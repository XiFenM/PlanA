NVIDIA has been transforming computer graphics, PC gaming, and accelerated computing for more than 25 years. It’s a unique legacy of innovation that’s fueled by great technology—and amazing people. Today, we’re tapping into the unlimited potential of AI to define the next era of computing. An era in which our GPU acts as the brains of computers, robots, and self-driving cars that can understand the world. Doing what’s never been done before takes vision, innovation, and the world’s best talent. As an NVIDIAN, you’ll be immersed in a diverse, supportive environment where everyone is inspired to do their best work. Come join the team and see how you can make a lasting impact on the world.  
NVIDIA 已连续 25 多年改变计算机图形、PC 游戏和加速计算领域。这是一个独特的创新传统，由卓越的技术和杰出的人才驱动。如今，我们正挖掘 AI 的无限潜力，定义计算的新时代。在这个时代里，我们的 GPU 将成为计算机、机器人和自动驾驶汽车的大脑，使其能够理解世界。实现前所未有的成就需要远见、创新和全球顶尖人才。作为 NVIDIA 的一员，你将沉浸在多元、支持性的环境中，每个人都能激发出最佳表现。加入我们，看看你如何能为世界带来持久的影响。

We are seeking a highly-skilled Senior On-Device Model Inference Optimization Engineer to join our team and lead efforts in improving the performance and efficiency of AI models enabling the next generation of autonomous vehicles technology at NVIDIA!  
我们正在寻找一位技术精湛的资深设备端模型推理优化工程师加入我们的团队，领导改进 AI 模型性能和效率的工作，以支持 NVIDIA 下一代自动驾驶技术！

**What you'll be doing:  你的工作内容：**

- Develop and implement strategies to optimize AI model inference for on-device deployment.  
    开发和实施策略，以优化 AI 模型在设备端的推理部署。
    
- Employ techniques like pruning, quantization, and knowledge distillation to minimize model size and computational demands.  
    采用剪枝、量化和知识蒸馏等技术，以最小化模型大小和计算需求。
    
- Optimize performance-critical components using CUDA and C++.  
    使用 CUDA 和 C++优化性能关键组件。
    
- Collaborate with multi-functional teams to align optimization efforts with hardware capabilities and deployment needs.  
    与多职能团队合作，使优化工作与硬件能力和部署需求保持一致。
    
- Benchmark inference performance, identify bottlenecks, and implement solutions.  
    基准测试推理性能，识别瓶颈，并实施解决方案。
    
- Research and apply innovative methods for inference optimization.  
    研究和应用创新的推理优化方法。
    
- Adapt models for diverse hardware platforms and operating systems with varying capabilities.  
    适应不同能力硬件平台和操作系统上的模型。
    
- Create tools to validate the accuracy and latency of deployed models at scale with minimal friction.  
    创建工具，以最小的摩擦验证大规模部署模型的准确性和延迟。
    
- Recommend and implement model architecture changes to improve the accuracy-latency balance.  
    推荐并实施模型架构变更，以改善准确性与延迟的平衡。
    

**What we need to see:  
我们需要看到：**

- MSc or PhD in Computer Science, Engineering, or a related field, or equivalent experience.  
    计算机科学、工程或相关领域的硕士或博士学位，或同等经验。
    
- Over 10 years of confirmed experience specializing in model inference and optimization.  
    超过 10 年的确凿经验，专攻模型推理和优化。
    
- 15+ overall years of work experience in a relevant area  
    15 年以上相关领域工作经验
    
- Expertise in modern machine learning frameworks, particularly PyTorch, ONNX, and TensorRT.  
    精通现代机器学习框架，特别是 PyTorch、ONNX 和 TensorRT。
    
- Proven experience in optimizing inference for transformer and convolutional architectures.  
    在优化 Transformer 和卷积架构的推理方面有丰富经验。
    
- Strong programming proficiency in CUDA, Python, and C++.  
    精通 CUDA、Python 和 C++的编程。
    
- In-depth knowledge of optimization techniques, including quantization, pruning, distillation, and hardware-aware neural architecture search.  
    深入理解优化技术，包括量化、剪枝、蒸馏和硬件感知的神经架构搜索。
    
- Skilled in building and deploying scalable, cloud-based inference systems.  
    擅长构建和部署可扩展的云端推理系统。
    
- Passionate about developing efficient, production-ready solutions with a strong focus on code quality and performance.  
    热衷于开发高效、可生产使用的解决方案，并高度重视代码质量和性能。
    
- Meticulous attention to detail, ensuring precision and reliability in safety-critical systems.  
    注重细节，确保在安全关键系统中实现精确性和可靠性。
    
- Strong collaboration and communication skills for working optimally across multidisciplinary teams.  
    卓越的协作与沟通能力，能够在跨学科团队中高效工作。
    
- A proactive, diligent mentality with a drive to tackle complex optimization challenges.  
    积极主动、勤奋努力的心态，致力于解决复杂的优化挑战。
    

**Ways to stand out from the crowd:  
脱颖而出的方法：**

- Publications or industry experience in optimizing and deploying model inference at scale.  
    在规模化优化和部署模型推理方面的出版物或行业经验。
    
- Hands-on expertise in hardware-aware optimizations and accelerators such as GPUs, TPUs, or custom ASICs.  
    在硬件感知优化和加速器（如 GPU、TPU 或定制 ASIC）方面的实际操作经验。
    
- Active contributions to open-source projects focused on inference optimization or machine learning frameworks.  
    在专注于推理优化或机器学习框架的开源项目中的积极贡献。
    
- Experience in designing and deploying inference pipelines for real-time or autonomous systems.  
    在为实时或自主系统设计和部署推理管道方面的经验。