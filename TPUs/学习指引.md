本章节的学习材料主要以Jax的《How to Scale Your Model: A Systems View of LLMs on TPUs》为核心，以翻译为主，中间也会夹杂一些译者的理解和笔记（会区分标出）。本书主要介绍TPU在大规模模型上的特性，也会与GPU进行对比。
链接： https://jax-ml.github.io/scaling-book/

让我们开始吧。
# 引言

训练一个大语言模型往往感觉像是炼丹术 (alchemy），但理解并优化一个模型并非如此。本系列旨在揭示 (demystify) 扩展语言模型的科学。
1. TPUs（和 GPUs）是如何工作的？他们之间是如何通信的？
2. LLMs是如何运行在真实硬件上的？
3. 如何在训练和推理中将模型并行化，从而使得他们能在大规模下高效运行？
4. 这个大语言模型的训练成本是多少？
5. 我需要多少内存来部署这个模型？
6. 什么是All Gather？
如果你对以上问题感兴趣，那么我相信接下来的内容将会对你有所帮助。

虽然深度学习的大部分内容依旧归结于（boil down to）某种黑魔法，但优化你的模型性能并非如此——即使是大规模模型！在模型性能优化中其实存在着相对简单的底层逻辑——从单个加速芯片到万卡加速集群——理解这些底层逻辑能让你做到许多有用的事：
- 估算（Ballpack）你的模型各个部分与理论最优值的接近程度。
- 在不同规模下就最佳的并行策略做出明智选择（make informed choices）（如何将计算分配到不同的设备上）
- 估算训练和运行大型Transformer模型的成本和时间。
- 设计能充分利用特定硬件特性的算法。
	- [Flash Attention](https://arxiv.org/abs/2205.14135) 
	- [Fast Transformer Decoding](https://arxiv.org/abs/1911.02150) 
	- [Data Movement Is All You Need](https://arxiv.org/abs/2007.00072) 
- 由对于当前算法性能瓶颈的清晰理解驱动，从而设计硬件。

**预期前提背景**：
我们将假设你对LLMs和Transformer架构有基本的理解，但不一定了解他们在规模化运作中的原理。希望你了解一些关于LLM训练的背景知识， 最好对Jax也有基本的熟悉度。
一些有用的背景阅读材料：
1. 关于Transformer架构的博客：[The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/) 
2. 最初的Transformer论文：[Attention Is All You Need](https://arxiv.org/abs/1706.03762) 



