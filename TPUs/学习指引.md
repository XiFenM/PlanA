本章节的学习材料主要以Jax的《How to Scale Your Model: A Systems View of LLMs on TPUs》为核心，以翻译为主，中间也会夹杂一些译者的理解和笔记（会区分标出）。本书主要介绍TPU在大规模模型上的特性，也会与GPU进行对比。
链接： https://jax-ml.github.io/scaling-book/

让我们开始吧。
# 引言

训练一个大语言模型往往感觉像是炼丹术 (alchemy），但理解并优化一个模型并非如此。本系列旨在揭示 (demystify) 扩展语言模型的科学。
1. TPUs（和 GPUs）是如何工作的？他们之间是如何通信的？
2. LLMs是如何运行在真实硬件上的？
3. 如何在训练和推理中将模型并行化，从而使得他们能在大规模下高效运行？
4. 这个大语言模型的训练成本是多少？
5. 我需要多少内存来部署这个模型？
6. 什么是All Gather？
如果你对以上问题感兴趣，那么我相信接下来的内容将会对你有所帮助。

虽然深度学习的大部分内容依旧归结于（boil down to）某种黑魔法，但优化你的模型性能并非如此——即使是大规模模型！在模型性能优化中其实存在着相对简单的底层逻辑——从单个加速芯片到万卡加速集群——理解这些底层逻辑能让你做到许多有用的事：
- 估算（Ballpack）你的模型各个部分与理论最优值的接近程度。
-  在不同规模上



