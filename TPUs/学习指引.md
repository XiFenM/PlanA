本章节的学习材料主要以Jax的《How to Scale Your Model: A Systems View of LLMs on TPUs》为核心，以翻译为主，中间也会夹杂一些译者的理解和笔记（会区分标出）。本书主要介绍TPU在大规模模型上的特性，也会与GPU进行对比。
链接： https://jax-ml.github.io/scaling-book/

让我们开始吧。
# 引言

训练一个大语言模型往往感觉像是炼丹术，但理解并优化一个模型并非如此。本系列旨在揭示扩展语言模型的科学。
1. TPUs（和 GPUs）是如何工作的？他们之间是如何通信的？
2. 如何在训练和推理中将模型并行化，从而使得他们能在大规模下高效运行？
3. 这个大语言模型的训练成本是多少？
4. 我需要多少内存来部署这个模型？
5. 什么是All Gather？
如果你对以上问题感兴趣，那么我相信接下来的内容将会对你有所帮助，并让你能做到以下的事情：
- 估算你的模型



