因为**互联带宽（Interconnect Bandwidth）** 和**通信库（Communication Library）** 往往是制约大模型扩展性的最大瓶颈，而非单卡的算力。

我们进入**第四部分：分布式计算 (Distributed Computing)**。

---

### 🌐 模块四：分布式计算与通信架构

**核心目标**：掌握如何将一个巨型模型（如 DeepSeek-V3 671B）拆解并映射到成千上万张卡上。理解从底层的通信原语（NCCL/DeepEP）到上层的并行策略（3D 并行/MoE）的完整技术栈，特别是如何在带宽受限的硬件上实现**计算与通信的重叠（Overlap）**。

**预计耗时**：6 - 8 周

#### 1. 学习路线图与核心知识点

我们将按照**通信层 -> 策略层 -> 编排层**的逻辑进行拆解：

第一层：通信原语与硬件抽象 (Communication Primitives)

时间：第 1-2 周

这是分布式系统的地基。如果底层通信慢，上层策略再好也没用。

- **1.1 集合通信 (Collective Communication)**
    
    - **核心原语**：深入理解 `All-Reduce` (同步梯度), `All-Gather` (收集参数), `Reduce-Scatter` (分散梯度), `All-to-All` (MoE 路由)。
        
    - **算法实现**：
        
        - **Ring All-Reduce**：适用于带宽受限但延迟不敏感的长链路场景（适合线性连接的自研芯片集群）。通信量恒定为 $2M(N-1)/N$。
            
        - **Tree All-Reduce**：适用于节点极多且消息较小的场景（如 LayerNorm 梯度），具有 $O(\log N)$ 的延迟优势。
            
    - **硬件映射**：理解 **RDMA (InfiniBand/RoCE)** 与 **节点内互联 (NVLink/PCIe)** 的区别。
        
- **1.2 DeepEP 与定制通信库**
    
    - **背景**：DeepSeek-V3 为了解决 MoE 路由带来的巨大 `All-to-All` 通信压力，开发了 **DeepEP**。
        
    - **关键技术**：
        
        - **SM-Free Overlap**：利用 GPU 的复制引擎（Copy Engine/DMA）在后台传输数据，而不占用流多处理器（SM）的计算资源。这对自研芯片非常重要：如果你的芯片 DMA 和 Compute 抢资源，性能会大打折扣。
            
        - **非对称带宽转发**：处理跨节点和节点内带宽不一致的问题，智能转发数据以防止网络拥塞。
            

---

第二层：并行策略与显存优化 (Parallelism Strategies)

时间：第 3-5 周

这一层是将模型“切开”的艺术。

- **2.1 3D 并行 (3D Parallelism)**
    
    - **数据并行 (DP/DDP)**：基础。
        
    - **张量并行 (TP - Tensor Parallelism)**：切分矩阵乘法（Megatron-LM）。这需要极高的节点内带宽，通常仅在单机内部使用。
        
    - **流水线并行 (PP - Pipeline Parallelism)**：将层切分到不同节点。重点掌握 **1F1B 调度** 和 **气泡（Bubble）** 优化。Interleaved 1F1B 可以显著减小气泡。
        
- **2.2 显存优化 (ZeRO / FSDP)**
    
    - **ZeRO 系列**：理解 ZeRO-1 (分片优化器), ZeRO-2 (分片梯度), ZeRO-3 (分片参数)。
        
    - **FSDP (Fully Sharded Data Parallel)**：PyTorch 的原生实现。
        
        - **通信流**：`All-Gather` (前向/反向计算前收集参数) -> 计算 -> `Reduce-Scatter` (计算后立即释放参数并同步梯度)。
            
        - **Backward Prefetch**：在计算当前层梯度的同时，预取上一层的参数，实现重叠。
            
- **2.3 上下文并行 (CP - Context Parallelism)**
    
    - 针对长序列（Long Context）场景（如 128k+ token）。不同于 TP 切分算子，CP 沿着 Sequence 维度切分 KV Cache 和 Attention 计算，解决了单卡显存无法容纳长序列的问题。
        

---

第三层：分布式编排与调度 (Orchestration)

时间：第 6 周

这一层是管理成千上万个进程的操作系统。

- **3.1 Ray 生态系统**
    
    - **Ray Core**：理解 **Tasks** (无状态函数) 和 **Actors** (有状态服务) 的编程模型。
        
    - **Ray Train**：负责 **Gang Scheduling**（帮派调度），确保所有训练 Workers 同时启动并初始化通信组，处理节点故障恢复。
        
    - **异构流水线**：在推理场景中，Ray 可以将预处理（Tokenization）放在 CPU Actor 上，模型推理放在 GPU Actor 上，实现流水线并行。
        

---

#### 2. 精选学习资料推荐

**A. 必读核心材料 (High Priority)**

1. **上传文件：《指引.md》**
    
    - **重点章节**：第 4 章节“分布式计算与通信架构”。
        
    - **内容**：详细解析了 NCCL 原语、DeepEP 的 SM-Free 机制、Ray 的调度模型以及 MoE 的通信瓶颈。
        
2. **上传文件：《初级成长计划.md》**
    
    - **重点章节**：第 4 章节“大规模分布式训练架构深度解析”。
        
    - **内容**：提供了 Ring All-Reduce 的数学公式、ZeRO/FSDP 的显存节省比例以及 PP 的气泡问题分析。
        
3. **Megatron-LM 论文 (NVIDIA)**
    
    - 它是 3D 并行的鼻祖，详细描述了 TP 的切分逻辑。
        

**B. 进阶阅读**

1. **DeepSeek-V3 Technical Report**
    
    - 重点阅读关于 **DeepEP** 和 **Dual-Pipe MoE**（双流水线 MoE）的部分，这是当前最前沿的通信优化实践。
        
2. **PyTorch FSDP 官方教程**
    
    - 理解 `FullStateDict` 和 `ShardedStateDict` 的区别。
        

---

#### 3. 动手实践任务 (Actionable Tasks)

在没有大规模集群的情况下，你依然可以通过模拟来学习：

- **任务一：多进程模拟 All-Reduce (Simulate Collective)**
    
    - **操作**：使用 PyTorch 的 `torch.multiprocessing` 在单机上启动 4 个进程，使用 `torch.distributed.all_reduce` 模拟梯度同步。
        
    - **观测**：使用 `nsys` 或 `torch.profiler` 抓取 Trace，观察通信算子（如 `ncclAllReduce`）在时间轴上是如何阻塞计算的。
        
- **任务二：FSDP 显存分析 (Memory Profiling)**
    
    - **操作**：微调一个简单的 Llama 模型（如 1B 参数），分别使用 `DDP` 和 `FSDP`。
        
    - **分析**：记录峰值显存占用。验证 ZeRO-3 是否真的节省了显存？开启 `backward_prefetch` 后，计算时间是否减少了？显存是否增加了？（验证《初级成长计划》中提到的重叠与显存的权衡）。
        
- **任务三：手写简单的 Ray Actor 系统**
    
    - **操作**：编写一个 Ray 脚本，创建一个 "Parameter Server" Actor 和多个 "Worker" Actors。Workers 计算完更新推送到 Server，Server 平均后发回 Workers。
        
    - **目的**：理解参数服务器（PS）架构与 All-Reduce 架构的区别，以及 Ray 如何轻松管理这些异构角色。
        

---

**下一步**

当你掌握了如何将模型分散到千卡集群后，最后一块拼图就是如何让这些分散的代码运行得极致高效——这就是**第五部分：编译器 (Compilers)**。我们将探讨 MLIR 如何作为通用语言连接你的 PyTorch 图和自研芯片的底层指令。

关于 DeepEP 的“SM-Free”机制，或者 Ray 在推理中的流水线应用，你有什么特别想深入了解的吗？