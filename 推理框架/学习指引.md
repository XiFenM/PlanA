### 🚀 模块二：高效推理框架 (High-Performance Inference Frameworks)

**核心目标**：从单卡推理扩展到复杂的分布式服务系统。掌握如何“压榨”硬件的每一分显存带宽（Memory Bandwidth），以及如何让模型“听话”地输出结构化数据。针对自研芯片，重点在于理解这些机制的**逻辑实现**，以便在你的硬件上复刻或优化。

**预计耗时**：8 - 10 周（内容极多，建议分模块攻破）

#### 1. 学习路线图与核心知识点

我们将这些庞杂的知识点重新梳理为四个逻辑层级：**内核层（显存与模型）** -> **调度层（速度优化）** -> **功能层（交互与约束）** -> **服务层（接口与网关）**。

---

第一层：内核与显存管理 (Core Engine & Memory)

时间：第 1-2 周

这一层是推理引擎的地基，决定了系统的并发上限。

- **1.1 KV Cache 显存管理机制**
    
    - **核心痛点**：解决显存碎片化与利用率低的问题。
        
    - **PagedAttention (vLLM)**：理解逻辑块到物理块的映射表（Block Table），以及它如何实现零拷贝的 Copy-on-Write。
        
    - **RadixAttention (SGLang) / Prefix Caching**：学习如何利用基数树（Radix Tree）管理 KV Cache，实现多轮对话和 System Prompt 的前缀复用。
        
        - _自研芯片思考_：你的硬件 DMA 引擎是否支持非连续内存的高效搬运？
            
    - **Hybrid Model (混合存储)**：当显存不足时，如何高效地将 KV Cache 卸载（Offload）到 CPU 内存甚至 NVMe SSD，并在需要时预取（Prefetch）。
        
- **1.2 模型适配与微调 (LoRA)**
    
    - **LoRA (Low-Rank Adaptation)**：在推理侧，重点是 **Multi-LoRA Serving**。即在一个 Base Model 上同时服务多个不同的 LoRA Adapter 请求。
        
    - **实现挑战**：如何在 Kernel 层面高效地将不同的 $A \times B$ 矩阵加到主干权重上，而不造成 Batching 的断裂。
        

---

第二层：高级调度与速度优化 (Advanced Scheduling & Acceleration)

时间：第 3-5 周

这一层旨在突破自回归生成的串行瓶颈。

- **2.1 PD 分离 (Prefill-Decode Disaggregation)**
    
    - **概念**：将计算密集型的 Prefill 阶段和访存密集型的 Decode 阶段拆分到不同的机器/卡上。
        
    - **关键技术**：
        
        - **Chunked Prefill**：将长 Prompt 拆分处理，避免阻塞 Decode 请求，平滑延迟。
            
        - **KV Cache 传输**：在 Prefill 机器计算完 KV 后，如何通过高速网络（RDMA/DeepEP）将其传输给 Decode 机器。
            
- **2.2 投机解码 (Speculative Decoding)**
    
    - **原理**：利用“验证比生成快”的特性，打破串行生成限制。
        
    - **变体**：
        
        - **Draft Model**：使用小模型生成草稿。
            
        - **Eagle / Eagle 2/3**：使用特征层面的回归头来预测后续 Token 特征，比独立小模型更轻量。
            
        - **MTP (Multi-Token Prediction)**：DeepSeek-V3 的原生多 Token 预测头，训练时即集成，推理时作为高接受率的 Draft 来源。
            
    - **树状注意力 (Tree Attention)**：验证阶段不再是验证单链，而是验证一棵 Token 树，这需要特殊的 Attention Mask 处理。
        
- **2.3 MoE 模型优化**
    
    - **DeepSeek-V3/Mixtral 实战**：理解细粒度专家（Fine-grained Experts）和共享专家（Shared Experts）的路由逻辑。
        
    - **DeepEP 通信**：学习专为 MoE 设计的非对称带宽转发和计算通信重叠（Overlap）机制。
        

---

第三层：功能增强与交互 (Functional Features)

时间：第 6-7 周

这一层让大模型不仅能“说话”，还能“办事”和“遵守规则”。

- **3.1 结构化输出与约束解码 (Structured Output)**
    
    - **Constraint Decoding**：如何强迫模型输出符合 JSON Schema 或正则表达式。
        
    - **Compressed FSM (SGLang)**：重点学习 SGLang 如何将正则编译为有限状态机（FSM），并利用压缩 FSM 结合 GPU 并行计算来加速 Token Masking 的过程。
        
        - _关键点_：传统的逐 Token 掩码是 CPU 瓶颈，如何将其下沉到 GPU 或 C++ 层？
            
- **3.2 Tool Calling & Agent API**
    
    - **OpenAI / Anthropic API 格式**：理解 ChatML 格式，以及 Tool Use 的特殊 Token 序列。
        
    - **MCP (Model Context Protocol)**：这是一种新兴的标准，用于标准化 AI 模型与外部数据/工具的连接。你需要关注它定义的 Client-Host-Server 架构。
        
    - **Harmony Format**：这通常指统一不同模型输入输出差异的中间格式，在多模型服务网关中非常重要。
        
- **3.3 多模态推理 (Multimodal)**
    
    - **Pipeline**：图像/视频 -> Encoder (ViT) -> Projector -> Embedding -> LLM。
        
    - **VLLM 实现**：vLLM 如何处理非文本 Input？核心是将 Image Features 视为占位符 Token 插入到 Sequence 中，并处理 Cross-Attention。
        

---

第四层：服务架构与语言实现 (Serving Layer)

时间：第 8 周

这一层是系统的“门面”，决定了系统的稳定性和易用性。

- **4.1 HTTP 请求管理与语言选型**
    
    - **Python**：通常用于模型逻辑层（vLLM/SGLang 的核心逻辑多为 Python 控制流）。
        
    - **Rust**：高性能网关的首选（如 `Pingora`, `Router`）。用于处理高并发连接、鉴权、负载均衡。Rust 的内存安全和零开销抽象非常适合这一层。
        
    - **Golang**：控制平面（Control Plane）的霸主（如 K8s Operator, 调度器）。适合处理业务逻辑复杂的任务分发。
        
    - **C++**：底层引擎（Torch C++, TensorRT-LLM）。
        
    - **架构模式**：学习 Reactor 模式在推理服务中的应用。
        

---

#### 2. 精选学习资料推荐

**A. 核心架构与源码**

1. **vLLM 官方文档与论文**：
    
    - Paper: _Efficient Memory Management for Large Language Model Serving with PagedAttention_。
        
    - Code: 重点阅读 `vllm/core/scheduler.py` (调度) 和 `vllm/engine/`。
        
2. **SGLang 论文与博客**：
    
    - Paper: _SGLang: Efficient Execution of Structured Language Model Programs_。
        
    - Blog: LMSYS 官方关于 RadixAttention 和 Compressed FSM 的技术博客。
        

B. 前沿技术专题

3. DeepSeek 技术报告：

* DeepSeek-V3 Technical Report：这是学习 MTP (Multi-Token Prediction) 和 DeepEP (MoE 通信) 的最佳一手资料。

* DeepSeek-R1：了解强化学习后训练对推理模式的影响（如长思维链）。

4. Speculative Decoding 综述：

* 搜索关于 "Eagle" (Extrapolating the Next Generation Large Language Models) 的论文。

C. 协议与标准

5. Model Context Protocol (MCP)：访问 MCP 的官方 GitHub 或文档，理解其协议规范。

6. OpenAI API Reference：详细阅读 "Chat Completions API" 中关于 tools 和 response_format (JSON mode) 的部分。

---

#### 3. 动手实践任务 (Actionable Tasks)

- **任务一：手写前缀树缓存 (Mini RadixCache)**
    
    - **目标**：理解 SGLang 的核心。
        
    - **操作**：用 Python 实现一个简单的 Radix Tree，模拟插入 Prompt、查找最长公共前缀、并进行 LRU 淘汰的过程。
        
    - **自研芯片适配**：思考如何在你的硬件上持久化这些树节点对应的显存块。
        
- **任务二：实现约束解码掩码 (Constrained Masking)**
    
    - **目标**：理解结构化输出。
        
    - **操作**：使用 `outlines` 或 `guidance` 库。编写一个简单的脚本，限制 LLM 只能输出符合特定 Regex（如邮箱格式）的字符串。
        
    - **深入**：查看其源码，看它是如何在每一步生成时修改 Logits 的（将不符合正则的 Token logit 设为 $-\infty$）。
        
- **任务三：模拟 PD 分离的数据流**
    
    - **目标**：理解 KV Cache 传输。
        
    - **操作**：在两台机器（或两个进程）间，模拟一个 "Prefill 实例" 和一个 "Decode 实例"。
        
    - **挑战**：Prefill 实例计算完 KV 后，将其序列化并通过 TCP/Socket 发送给 Decode 实例，Decode 实例加载后继续生成。记录传输延迟与计算时间的比例。
        

---

想针对 PD 分离或结构化输出先展开讨论吗？